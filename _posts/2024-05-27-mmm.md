### 딥러닝과 이진 분류에서의 손실 함수

1. **딥러닝 학습 과정**
   - 입력 데이터를 네트워크에 넣고 예측값 계산
   - 예측값과 실제값 비교하여 손실 계산
   - 손실 최소화 위해 가중치(weight)와 편향(bias) 조정

2. **손실 함수와 활성화 함수**
   - 회귀 문제: 예측값과 실제값 차이 최소화 (예: 평균 제곱 오차 MSE)
   - 이진 분류 문제: 이진 교차 엔트로피 손실 함수 사용, 예측 확률과 실제 라벨 간 차이 최소화
   - 시그모이드 함수: 이진 분류 문제에서 출력값을 0과 1 사이의 확률로 변환

3. **이진 교차 엔트로피 손실 함수**
   - 수식:
     $$
     L = -\sum_{i=1}^{N} \left( y_i \log(q_i) + (1 - y_i) \log(1 - q_i) \right)
     $$
   - \( y_i \): 실제 라벨 (0 또는 1)
   - \( q_i \): 모델의 예측 확률

4. **로그 사용하는 이유**
   - 곱셈을 덧셈으로 변환
   - 매우 작은 값의 곱셈으로 인한 수치적 불안정성 완화

5. **결합 확률과 손실 함수**
   - 여러 샘플의 경우, 각 샘플의 예측 확률을 곱하여 결합 확률 구함:
     $$
     P(\mathbf{y} | \mathbf{q}) = \prod_{i=1}^{N} q_i^{y_i} \cdot (1 - q_i)^{1 - y_i}
     $$
   - 결합 확률에 로그 취함:
     $$
     \log P(\mathbf{y} | \mathbf{q}) = \log \left( \prod_{i=1}^{N} q_i^{y_i} \cdot (1 - q_i)^{1 - y_i} \right) = \sum_{i=1}^{N} \left( y_i \log(q_i) + (1 - y_i) \log(1 - q_i) \right)
     $$

6. **손실 함수 최소화**
   - 딥러닝의 목표는 항상 손실 함수를 최소화하는 것
   - 손실 함수 최소화로 모델의 예측이 실제 라벨과 최대한 일치하게 함
   - 이진 교차 엔트로피 손실 함수의 경우, 결합 확률을 최대화하는 것이 손실 함수의 로그 값의 음수를 최소화하는 것과 동일

### 결론

- 딥러닝의 학습 목표는 항상 손실 함수를 최소화하는 것
- 이진 교차 엔트로피 손실 함수는 이진 분류 문제에서 사용되며, 모델의 예측 확률이 실제 라벨과 얼마나 잘 맞는지를 평가
- 시그모이드 함수는 이진 분류 문제에서 출력값을 확률로 변환하는 데 사용
- 손실 함수에 로그를 취하는 이유는 곱셈을 덧셈으로 변환하고 수치적 안정성을 높이기 위해서
- 결합 확률을 최대화하는 것은 손실 함수를 최소화하는 것과 동일한 목표를 가짐

