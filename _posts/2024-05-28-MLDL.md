# 지도 학습
- **정답**을 알고 있다!

- 지도 학습의 예)
  - **회귀 (regression)**
  - **분류 (classification)**

# 비지도 학습
- **정답**을 모른다

- 지도 학습의 예)
  - **군집화 (K-means, DBSCAN)**
  - **차원 축소 (데이터 전처리: PCA, SVD)**
  - **GAN**




# 자기지도 학습
- 정답을 알고 있는 데이터가 너무 적다
- 진짜 풀려고 했던 문제 말고 다른 문제를 새롭게 정의해서 먼저 풀어본다!
#### 순서
- 1.pretext task 학습으로 pre-training<br>
  2.downstream task(분류)를 풀기 위해 transfer learning함


# 강화학습
- Agent : 강아지, 흑돌
- Reward : 간식, 승리
- Environment : 보호자, 흰돌
- Action : 손, 수
- State : 바둑판, 위치
- Q-function : Q(s, q) 입력이 두개다 : 어떻게 했을때 얼마나 좋다 저장
- Episode : 판, 시도
- Q-learning : 
- Exploration : discount factor을 곱해줘서 최적의 길을 찾는다

# ******************
# 중간 내용 강의 다시 보기
# ******************

 # non-linear activation - unit step function
- 출력 1 or 0
- 선형 분류(경계가 선형)로 쓰일 수 있다. 근데 결과가 선형은 X
- **퍼셉트론** : hidden layer 없이 unit step function을 activation으로 사용
- 1. 미분불가(gradient descent 못씀)
  2. 너무 빡빡하게 분리(모 아니면 도)
이러한 것을 보완하기 위해 sigmoid function이 나옴


# sigmoid function
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
- 1. 전구간 미분 가능
  2. 부드러운 분류 가능
- 확률로 해석 가능
- 합리적인 분류 기준 선을 찾게 됨

# CSS
- 위치, 배치가 중요하다
- 컨볼루션 = 필터링 = 특징 추출!
- 같은 weight set으로 쭉
- 위치정보를 기억함(담당 노드)
- 필터(커널)이 Weight, bias인데 이 값이 학습 파라미터들이다!
## conv layer가 FC layer와 다른 점
- 주변만 연결한다
- weight 재사용(쭉 긁으면서 스캔)
- 여러 종류의 필터로 여러 종류의 특징 추출
- 각 필터가 어떤 특징을 추출하는 지 머신이 학습!

## 컬러 사진이면? (RGB 3개 채널)
- 3개 채널 들어오면 필터도 무조건 3개

## Padding
필터 convolution했을때 크기 안작아지게 주위에 0 두르는 것

## Stride
몇 칸씩 옮기면서 곱할지

## Pooling
-사이즈를 줄여 넓은 범위를 대표할 수 있게 함(또 다른 **layer** 다)
- **Max pooling**<br>
 최대한으로 줄여주는 것
- **Average pooling**<br>
 그냥 값들을 평균 내줌
- 각 channel마다 해서 channel 개수는 유지된다












